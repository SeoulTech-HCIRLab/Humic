#!/usr/bin/env python3
# encoding: utf-8

""" ROS """
import rospy
import rospkg

""" python Library """
import os
import sys
import time
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))
import numpy as np
import yaml

np.random.seed(14512281)

""" Environment """
from src.humic_push_env import HumicPushEnv
from src.ddpg import DDPGagent

import matplotlib.pyplot as plt

def plotLearning(reward, x=None, window=10):
    dirPath = rospkg.RosPack().get_path('humic_rl')
    filename = os.path.join(dirPath, 'checkpoint/ddpg/20210113/2/humic_ddpg.png')
    N = len(reward)
    running_avg = np.empty(N)
    for t in range(N):
	    running_avg[t] = np.mean(reward[max(0, t-window):(t+1)])
    if x is None:
        x = [i for i in range(N)]
    plt.ylabel('Reward Avg')       
    plt.xlabel('Episode')                     
    plt.plot(x, running_avg)
    plt.savefig(filename)

def DDPGmain():
    rospy.init_node('humicpush')

    dirPath = rospkg.RosPack().get_path('humic_rl')
    filePath = os.path.join(dirPath, 'config/param.yaml')
    params = yaml.load(open(filePath, 'r'))

    EPISODE = params['episode']
    STEP = params['max_step']
    OBJ_CHANGE = (EPISODE*params['obj_change'])
    MODEL_SAVE = (EPISODE*params['model_save'])
    # epslion = params['epslion'] # e-greedy random action
    layer1 = params['layer1']
    layer2 = params['layer2']
    actor_lr = params['actor_lr']
    critic_lr = params['critic_lr']
    buffer_size = params['buffer_size']
    batch_size = params['batch_size']
    gamma = params['gamma']
    tau = params['tau']
    # theta =  params['theta']
    # sigma = params['sigma']
    # dt = params['dt']
    # x0 = params['x0']

    env = HumicPushEnv(max_step=STEP)
    total_step = 0

    obs_dim = env.observation_space.shape[0]
    action_dim = env.action_space.shape[0]
    
    agent = DDPGagent(env=env, obs_dim=obs_dim, action_dim=action_dim, gamma=gamma, tau=tau,
                        actor_lr=actor_lr, critic_lr=critic_lr, hidden1=layer1, hidden2=layer2, 
                        batch_size=batch_size, buffer_size=buffer_size)

    #agent.load_models()
    
    reward_history = []
    reward_avg = []
    for episode in range(1,EPISODE+1):
        if episode == 1:
            init_obj = True
            change_obj = False
        elif episode != 1 and episode % OBJ_CHANGE == 0:
            init_obj = False
            change_obj = True
        else:
            init_obj = False
            change_obj = False
        obs = env.reset(init_position=init_obj, change_position=change_obj)
        time.sleep(3)
        done = False
        total_reward = 0
        t_ = 0 
        # for timesteps in range(TIMESTEPS):
        while not done:
            # if np.random.random_sample() < epslion:
            #     act = agent.randomAction()
            # else:
            #     act = agent.getAction(obs)
            act = agent.getAction(obs)
            new_state, reward, done, info = env.step(act)
            agent.remember(obs, act, reward, new_state, int(done))
            agent.learn()
            total_reward += reward
            t_+=1
            total_step+=1
            obs = new_state
            if done:
                break
        reward_history.append(total_reward)
        reward_avg.append((total_reward/t_))
        if episode % MODEL_SAVE == 0:
            agent.save_models()

        print('episode: %d'%episode, 'timesteps: %d'%t_, 'total reward: %.3f'%total_reward, 'avg reward: %.3f'%(total_reward/t_), 'Get target: %d'%env.get_target)

    plotLearning(reward=reward_avg, window=200)
    env.pauseSim()
    print("Training Finished, Total Step:{}".format(total_step))
    rospy.spin()

if __name__ == '__main__':
    DDPGmain()