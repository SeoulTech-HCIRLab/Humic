#!/usr/bin/env python3
# encoding: utf-8

""" ROS """
import rospy
import rospkg

""" python Library """
import os
import sys
import time
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))

import ray
import ray.rllib.agents.ppo as ppo
from ray.tune.registry import register_env
import shutil

""" Environment """
from src.humic_push_env import HumicPushEnv

## start Ray

def env_creator(env_config):
    return HumicPushEnv()

def main():
    rospy.init_node('humicpush')
    ray.shutdown()
    ray.init(ignore_reinit_error=True)

    ## set up directories to log results and checkpoints

    CHECKPOINT_ROOT = "/home/young/catkin_ws/src/humic_rl/nodes/tmp/ppo/humic"
    shutil.rmtree(CHECKPOINT_ROOT, ignore_errors=True, onerror=None)

    # ray_results = "{}/ray_results/".format(os.getenv("HOME"))
    ray_results = "/home/young/catkin_ws/src/humic_rl/nodes/ray_results/"
    shutil.rmtree(ray_results, ignore_errors=True, onerror=None)

    ## configure the CartPole environment

    # SELECT_ENV = "CartPole-v1"
    # env = HumicPushEnv()

    # register_env("humic", env_creator)

    config = ppo.DEFAULT_CONFIG.copy()
    config["log_level"] = "WARN"

    # ray.init()
    agent = ppo.PPOTrainer(config={"env_config":{}}, env=HumicPushEnv)

    ## train a policy with PPO

    N_ITER = 30
    s = "{:3d} reward {:6.2f}/{:6.2f}/{:6.2f} len {:6.2f} saved {}"

    for n in range(N_ITER):
        result = agent.train()
        file_name = agent.save(CHECKPOINT_ROOT)

        print(s.format(
            n + 1,
            result["episode_reward_min"],
            result["episode_reward_mean"],
            result["episode_reward_max"],
            result["episode_len_mean"],
            file_name
            ))

    ## examine the trained policy

    policy = agent.get_policy()
    model = policy.model
    print(model.base_model.summary())
    rospy.spin()

if __name__ == '__main__':
    main()