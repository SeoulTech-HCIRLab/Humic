#!/usr/bin/env python3
# encoding: utf-8
# -------------------------------
# Humic Push Task
# Author:  Kim Young Gi(HCIR Lab.)
# Date: 2021. 02. 02
# -------------------------------
""" ROS """
import rospy
import rospkg

""" python Library """
import os
import sys
import time
import yaml
import numpy as np
import matplotlib.pyplot as plt
np.random.seed(14512281)

""" Environment """
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))
from src.humic_push_env import HumicPushEnv
from src.ddpg import DDPGagent
from src.ppo import PPOagent

DDPGcheckpoPath = 'checkpoint/ddpg/20210204' # checkpoint Path
# PPOcheckpoPath = 'checkpoint/ppo/20210202'

def rewardPlot(rewards):
    episode = len(rewards)
    x = np.arange(1, episode+1, 1)
    dirPath = rospkg.RosPack().get_path('humic_rl')
    filePath = os.path.join(dirPath, DDPGcheckpoPath, 'humic_ddpg_reward_{}.png'.format(episode))
    # filePath = os.path.join(dirPath, PPOcheckpoPath, 'humic_ppo_reward_{}.png'.format(episode))
    plt.ylabel('Reward Avg')       
    plt.xlabel('Episode')                     
    plt.plot(x, rewards, 'b-')
    plt.savefig(filePath)

def DDPGtrain():
    rospy.init_node('humicpush')

    dirPath = rospkg.RosPack().get_path('humic_rl')
    paramsPath = os.path.join(dirPath, 'config/param_ddpg.yaml')
    params = yaml.load(open(paramsPath, 'r'))

    EPISODE = params['episode']
    MODEL_SAVE = params['model_save']
    hidden1 = params['hidden1']
    hidden2 = params['hidden2']
    actor_lr = params['actor_lr']
    critic_lr = params['critic_lr']
    buffer_size = params['buffer_size']
    batch_size = params['batch_size']
    gamma = params['gamma']
    tau = params['tau']
    theta =  params['theta']
    sigma = params['sigma']
    dt = params['dt']

    env = HumicPushEnv()

    obs_dim = env.observation_space.shape[0]
    action_dim = env.action_space.shape[0]
    action_bound = env.action_bound
    print("DDPG Train, obs_dim: {}\t action_dim: {}\t action_bound: {}".format(obs_dim, action_dim, action_bound))
    
    checkpointPath = os.path.join(dirPath, DDPGcheckpoPath)
    agent = DDPGagent(obs_dim=obs_dim, action_dim=action_dim, action_bound=action_bound, gamma=gamma, tau=tau,
                        actor_lr=actor_lr, critic_lr=critic_lr, hidden1=hidden1, hidden2=hidden2,
                        batch_size=batch_size, buffer_size=buffer_size, checkpoint_path=checkpointPath,
                        theta=theta, sigma=sigma, dt=dt)

    PLOT = EPISODE*0.2
    reward_history = []
    total_step = 0

    for episode in range(1,EPISODE+1):
        obs = env.reset()
        sum_reward = 0
        t_ = 0
        done = False
        while not done:
            action = agent.getAction(obs)
            new_state, reward, done, _ = env.step(action)
            agent.remember(obs, action, reward, new_state, int(done))
            agent.learn()
            sum_reward += reward
            obs = new_state
            t_+=1
            total_step+=1
            if done:
                break
        
        avg_reward = sum_reward/t_
        reward_history.append(avg_reward) # store average reward
        
        print('episode: %d'%episode, 'avg_reward: %.3f'%avg_reward, 'Get target: %d'%env.get_target, 'total step: %d'%total_step)
        
        if episode % MODEL_SAVE == 0:
            agent.save_models()
        
        if episode % PLOT == 0:
            rewardPlot(rewards=reward_history)

    print("End Learning")
    env.pauseSim()
    # rospy.spin()


def DDPGtest():
    rospy.init_node('humicpush')

    dirPath = rospkg.RosPack().get_path('humic_rl')
    paramsPath = os.path.join(dirPath, 'config/param_ddpg.yaml')
    params = yaml.load(open(paramsPath, 'r'))

    EPISODE = params['episode']
    MODEL_SAVE = params['model_save']
    hidden1 = params['hidden1']
    hidden2 = params['hidden2']
    hidden3 = params['hidden3']
    actor_lr = params['actor_lr']
    critic_lr = params['critic_lr']
    buffer_size = params['buffer_size']
    batch_size = params['batch_size']
    gamma = params['gamma']
    tau = params['tau']
    theta =  params['theta']
    sigma = params['sigma']
    dt = params['dt']

    env = HumicPushEnv()
    total_step = 0

    obs_dim = env.observation_space.shape[0]
    action_dim = env.action_space.shape[0]
    action_bound = env.action_bound
    print("DDPG Test, obs_dim: {}\t action_dim: {}\t action_bound: {}".format(obs_dim, action_dim, action_bound))
    
    checkpointPath = os.path.join(dirPath, DDPGcheckpoPath)
    agent = DDPGagent(obs_dim=obs_dim, action_dim=action_dim, action_bound=action_bound, gamma=gamma, tau=tau,
                        actor_lr=actor_lr, critic_lr=critic_lr, hidden1=hidden1, hidden2=hidden2, hidden3=hidden3,
                        batch_size=batch_size, buffer_size=buffer_size, checkpoint_path=checkpointPath,
                        theta=theta, sigma=sigma, dt=dt)
    
    agent.load_models()

    for episode in range(1,EPISODE+1):
        obs = env.reset()
        done = False
        print("Episode: {}".format(episode))
        while not done:
            action = agent.getAction(obs)
            new_state, reward, done, _ = env.step(action)
            obs = new_state
            if done:
                break

    print("End Test")
    env.pauseSim()
    # rospy.spin()

def PPOtrain():
    rospy.init_node('humicpush')

    dirPath = rospkg.RosPack().get_path('humic_rl')
    paramsPath = os.path.join(dirPath, 'config/param_ppo.yaml')
    params = yaml.load(open(paramsPath, 'r'))

    EPISODE = params['episode']
    MODEL_SAVE = params['model_save']
    update_timestep = params['update_timestep'] # update policy every n timesteps
    hidden1 = params['hidden1']
    hidden2 = params['hidden2']
    n_epochs = params['n_epochs']               # update policy for K epochs
    clipping = params['clipping']               # clip parameter for PPO
    gamma = params['gamma']                     # discount factor
    lr = params['learning_rate']                # parameters for Adam optimizer
    betas = (0.9, 0.999)

    env = HumicPushEnv()

    obs_dim = env.observation_space.shape[0]
    action_dim = env.action_space.shape[0]
    action_bound = env.action_bound
    action_std = round((1./action_dim),2)                  # constant std for action distribution (Multivariate Normal)

    print("PPO Train, obs_dim: {}\t action_dim: {}\t action_bound: {}\t action_std: {}".format(obs_dim, action_dim, action_bound, action_std))
    
    checkpointPath = os.path.join(dirPath, PPOcheckpoPath)
    agent = PPOagent(obs_dim=obs_dim, action_dim=action_dim, action_std=action_std, action_bound=action_bound, hidden1=hidden1, hidden2=hidden2,
                        lr=lr, betas=betas, gamma=gamma, n_epochs=n_epochs, clipping=clipping, checkpoint_path=checkpointPath)

    PLOT = EPISODE*0.2
    reward_history = []
    total_step = 0
    time_step = 0

    for episode in range(1,EPISODE+1):
        obs = env.reset()
        sum_reward = 0
        t_ = 0
        done = False
        while not done:
            time_step+=1
            t_+=1
            total_step+=1

            action = agent.getAction(obs)
            obs, reward, done, _ = env.step(action)
            agent.memorize(reward, done)
            
            if time_step % update_timestep == 0:
                agent.update()
                time_step = 0
            
            sum_reward+=reward

            if done:
                # print("{}Episode Step{}".format(episode, env.step_ctr))
                break

        avg_reward = sum_reward/t_
        reward_history.append(avg_reward) # store average reward
        
        print('episode: %d'%episode, 'avg_reward: %.3f'%avg_reward, 'Get target: %d'%env.get_target, 'total step: %d'%total_step)
        
        if episode % MODEL_SAVE == 0:
            agent.save_models()
        
        if episode % PLOT == 0:
            rewardPlot(rewards=reward_history, episode=episode)

    print("End Learning")
    env.pauseSim()
    # rospy.spin()

def PPOtest():
    rospy.init_node('humicpush')

    dirPath = rospkg.RosPack().get_path('humic_rl')
    paramsPath = os.path.join(dirPath, 'config/param_ppo.yaml')
    params = yaml.load(open(paramsPath, 'r'))

    EPISODE = params['episode']
    MODEL_SAVE = params['model_save']
    update_timestep = params['update_timestep'] # update policy every n timesteps
    hidden1 = params['hidden1']
    hidden2 = params['hidden2']
    n_epochs = params['n_epochs']               # update policy for K epochs
    clipping = params['clipping']               # clip parameter for PPO
    gamma = params['gamma']                     # discount factor
    lr = params['learning_rate']                # parameters for Adam optimizer
    betas = (0.9, 0.999)

    env = HumicPushEnv()

    obs_dim = env.observation_space.shape[0]
    action_dim = env.action_space.shape[0]
    action_bound = env.action_bound
    action_std = 1./action_dim                  # constant std for action distribution (Multivariate Normal)

    print("PPO Test, obs_dim: {}\t action_dim: {}\t action_bound: {}\t action_std: {}".format(obs_dim, action_dim, action_bound, action_std))
    
    checkpointPath = os.path.join(dirPath, PPOcheckpoPath)
    agent = PPOagent(obs_dim=obs_dim, action_dim=action_dim, action_std=action_std, hidden1=hidden1, hidden2=hidden2,
                        lr=lr, betas=betas, gamma=gamma, n_epochs=n_epochs, clipping=clipping, checkpoint_path=checkpointPath)

    agent.load_models()

    reward_history = []
    sum_reward = 0
    total_step = 0
    time_step = 0
    t_ = 0
    done = False

    for episode in range(1,EPISODE+1):
        obs = env.reset()
        while not done:
            time_step+=1
            t_+=1
            total_step+=1

            action = agent.getAction(obs)
            obs, reward, done, _ = env.step(action)
            
            sum_reward+=reward

            if done:
                break

        avg_reward = sum_reward/t_
        reward_history.append(avg_reward) # store average reward
        sum_reward = 0
        t_ = 0
        done = False
        
        print('episode: %d'%episode, 'avg_reward: %.3f'%avg_reward,
                'Get target: %d'%env.get_target, 'total step: %d'%total_step)

    print("End Test")
    env.pauseSim()
    # rospy.spin()

if __name__ == '__main__':
    DDPGtrain()
    # DDPGtest()
    # PPOtrain()
    # PPOtest()